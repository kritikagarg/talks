{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Managing Imports\n",
    "from time import time\n",
    "import numpy as np\n",
    "import operator\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1084022f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This notebook comes with a pre-configured sparkContext called sc\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "#RDDs\n",
    "dummy_list = range(1000)\n",
    "#this is a list\n",
    "print type(dummy_list)\n",
    "rddlist = sc.parallelize(dummy_list)\n",
    "#this is a RDD\n",
    "print type(rddlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#More RDDs\n",
    "file_path = \"data/sequence.txt\"\n",
    "file_rdd = sc.textFile(file_path)\n",
    "json_rdd = sc.textFile(\"data/people.json\")\n",
    "# print json_rdd.collect()\n",
    "# print type(json_rdd.collect())\n",
    "#Spark supports text files, SequenceFiles, and any other Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in seconds) = 0.00355696678162\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "filtered_rdd = file_rdd.filter(lambda x: len(x) < 2)\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in seconds) = 5.95632791519\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "filtered_data = file_rdd.filter(lambda x: len(x) < 2).collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in seconds) = 0.00350093841553\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "mapped_rdd = file_rdd.map(lambda x: 2*x)\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken (in seconds) = 2.28691196442\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "mapped_data = file_rdd.map(lambda x: 2*x).collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<type 'list'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "<type 'list'>\n",
      "Conclusion: RDDs are lazy. They do nothing unless there is an action is called.\n"
     ]
    }
   ],
   "source": [
    "print type(filtered_rdd)\n",
    "print type(filtered_data)\n",
    "print type(mapped_rdd)\n",
    "print type(mapped_data)\n",
    "print \"Conclusion: RDDs are lazy. They do nothing unless there is an action is called.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Time taken (in seconds) = 0.129802942276\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).take(10))\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "Time taken (in seconds) = 0.553807973862\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).take(100000))\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n",
      "Time taken (in seconds) = 2.25160479546\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print len(file_rdd.map(lambda x: 2*x).filter(lambda x: len(x)>1).collect())\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lazy is better.\n"
     ]
    }
   ],
   "source": [
    "print \"Lazy is better.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to count the number of 1, 2, ... digit numbers.\n",
      "[(1, 9), (2, 90), (3, 900), (4, 9000), (5, 90000), (6, 900000), (7, 1)]\n",
      "Time taken (in seconds) = 1.79352688789\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print \"We want to count the number of 1, 2, ... digit numbers.\"\n",
    "file_path = \"data/sequence.txt\"\n",
    "file_rdd = sc.textFile(file_path) \n",
    "mapped_rdd = file_rdd.map(lambda a: (len(a), 1))\n",
    "count_rdd = mapped_rdd.reduceByKey(lambda a, b: a+b).sortByKey()\n",
    "print count_rdd.collect()\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We want to count the number of 1, 2, ... digit numbers.\n",
      "Time taken (in seconds) = 7.90541195869\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "print \"We want to count the number of 1, 2, ... digit numbers.\"\n",
    "mapped_data = np.asarray(map(lambda a: (len(a), 1), file_rdd.collect()))\n",
    "# count_map = mapped_rdd.reduceByKey(lambda a, b: a+b).sortByKey().collect()\n",
    "# print count_map\n",
    "end_time = time()\n",
    "print \"Time taken (in seconds) = \" + str(end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lets add all the numbers. We have two ways of doing that.\n",
      "Approach 1: update a counter variable\n",
      "Sum using first approach:  0\n",
      "Approach 2: use reduce operation\n",
      "Sum using second appraoch:  500000500000\n",
      "Which one is correct?\n"
     ]
    }
   ],
   "source": [
    "print \"Lets add all the numbers. We have two ways of doing that.\"\n",
    "print \"Approach 1: update a counter variable\"\n",
    "counter = 0\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter+=x\n",
    "mapped_rdd = file_rdd.map(lambda a: int(a))\n",
    "mapped_rdd.foreach(increment_counter)\n",
    "print \"Sum using first approach: \", counter\n",
    "print \"Approach 2: use reduce operation\"\n",
    "print \"Sum using second appraoch: \", mapped_rdd.reduce(operator.add)\n",
    "print \"Which one is correct?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Actual sum: ', 500000500000)\n"
     ]
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "mapped_rdd.foreach(lambda x: accum.add(x))\n",
    "print(\"Actual sum: \", accum.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far we talked about big data. What about structured big data?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.context.SQLContext at 0x7f10669611d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print \"So far we talked about big data. What about structured big data?\"\n",
    "sqlContext = SQLContext(sc)\n",
    "sqlContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|               email|first_name|gender| id|     ip_address|last_name|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|mross0@woothemes.com|      Mark|  Male|  1|  193.74.224.53|     Ross|\n",
      "|pbarnes1@state.tx.us|  Patricia|Female|  2|  118.198.82.58|   Barnes|\n",
      "|ghenry2@geocities.jp|   Gregory|  Male|  3| 11.202.213.200|    Henry|\n",
      "|mfernandez3@tmall...|      Mark|  Male|  4|   47.154.217.0|Fernandez|\n",
      "| jbaker4@auda.org.au|  Jennifer|Female|  5|   39.74.105.41|    Baker|\n",
      "|agilbert5@reddit.com|       Ann|Female|  6|199.219.100.148|  Gilbert|\n",
      "|hriley6@yolasite.com|    Howard|  Male|  7|   10.82.75.192|    Riley|\n",
      "|    sfisher7@bbb.org|    Samuel|  Male|  8|  167.52.36.254|   Fisher|\n",
      "|jmccoy8@examiner.com|    Jeremy|  Male|  9|  121.43.78.116|    Mccoy|\n",
      "|     jlopez9@ask.com|      Judy|Female| 10| 148.247.157.84|    Lopez|\n",
      "|rcollinsa@linkedi...|    Robert|  Male| 11|  211.141.37.30|  Collins|\n",
      "|cfullerb@comcast.net| Christina|Female| 12|  64.222.238.34|   Fuller|\n",
      "|jmitchellc@cocolo...|       Joe|  Male| 13|  210.158.29.11| Mitchell|\n",
      "|jgriffind@gizmodo...|     Jesse|  Male| 14|  80.114.100.93|  Griffin|\n",
      "|swarrene@economis...|      Sara|Female| 15|139.106.252.103|   Warren|\n",
      "|tpattersonf@exblo...|     Terry|  Male| 16| 131.113.230.48|Patterson|\n",
      "|    kcarrg@prlog.org|     Kelly|Female| 17|   42.81.36.143|     Carr|\n",
      "|nbellh@soundcloud...|    Nicole|Female| 18|    219.227.1.1|     Bell|\n",
      "|dmccoyi@blogspot.com|    Daniel|  Male| 19| 53.122.137.151|    Mccoy|\n",
      "|bhicksj@howstuffw...|  Benjamin|  Male| 20|  108.255.70.73|    Hicks|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "A DataFrame is the structured version of RDD. This is the familiar relational view of the data.\n"
     ]
    }
   ],
   "source": [
    "#dataframes\n",
    "df = sqlContext.read.json(\"data/people.json\")\n",
    "df.show()\n",
    "print \"A DataFrame is the structured version of RDD. This is the familiar relational view of the data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- email: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      "\n",
      "DF can infer schema on its own. We can always override the inferred schema.\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print \"DF can infer schema on its own. We can always override the inferred schema.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can not infer schema for type: <type 'unicode'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-dc35359ffec1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_rdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mtoDF\u001b[1;34m(self, schema, sampleRatio)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \"\"\"\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampleRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[0mRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msamplingRatio\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m             \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_infer_schema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_has_nulltype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shagun/devsetup/spark-1.5.1-bin-hadoop2.6/python/pyspark/sql/types.pyc\u001b[0m in \u001b[0;36m_infer_schema\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Can not infer schema for type: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mStructField\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_infer_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Can not infer schema for type: <type 'unicode'>"
     ]
    }
   ],
   "source": [
    "rdd_df = file_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| _1|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "| 20|\n",
      "+---+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- _1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df = file_rdd.map(lambda a: Row(a)).toDF()\n",
    "rdd_df.show()\n",
    "rdd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rdd_df = file_rdd.map(lambda a: Row(int(a))).toDF()\n",
    "# rdd_df.show()\n",
    "# rdd_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|               email|first_name|gender| id|     ip_address|last_name|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|mross0@woothemes.com|      Mark|  Male|  1|  193.74.224.53|     Ross|\n",
      "|pbarnes1@state.tx.us|  Patricia|Female|  2|  118.198.82.58|   Barnes|\n",
      "|ghenry2@geocities.jp|   Gregory|  Male|  3| 11.202.213.200|    Henry|\n",
      "|mfernandez3@tmall...|      Mark|  Male|  4|   47.154.217.0|Fernandez|\n",
      "| jbaker4@auda.org.au|  Jennifer|Female|  5|   39.74.105.41|    Baker|\n",
      "|agilbert5@reddit.com|       Ann|Female|  6|199.219.100.148|  Gilbert|\n",
      "|hriley6@yolasite.com|    Howard|  Male|  7|   10.82.75.192|    Riley|\n",
      "|    sfisher7@bbb.org|    Samuel|  Male|  8|  167.52.36.254|   Fisher|\n",
      "|jmccoy8@examiner.com|    Jeremy|  Male|  9|  121.43.78.116|    Mccoy|\n",
      "|     jlopez9@ask.com|      Judy|Female| 10| 148.247.157.84|    Lopez|\n",
      "|rcollinsa@linkedi...|    Robert|  Male| 11|  211.141.37.30|  Collins|\n",
      "|cfullerb@comcast.net| Christina|Female| 12|  64.222.238.34|   Fuller|\n",
      "|jmitchellc@cocolo...|       Joe|  Male| 13|  210.158.29.11| Mitchell|\n",
      "|jgriffind@gizmodo...|     Jesse|  Male| 14|  80.114.100.93|  Griffin|\n",
      "|swarrene@economis...|      Sara|Female| 15|139.106.252.103|   Warren|\n",
      "|tpattersonf@exblo...|     Terry|  Male| 16| 131.113.230.48|Patterson|\n",
      "|    kcarrg@prlog.org|     Kelly|Female| 17|   42.81.36.143|     Carr|\n",
      "|nbellh@soundcloud...|    Nicole|Female| 18|    219.227.1.1|     Bell|\n",
      "|dmccoyi@blogspot.com|    Daniel|  Male| 19| 53.122.137.151|    Mccoy|\n",
      "|bhicksj@howstuffw...|  Benjamin|  Male| 20|  108.255.70.73|    Hicks|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DF can be queried in two ways: API and sql queries\n",
      "First method: API\n"
     ]
    }
   ],
   "source": [
    "print \"DF can be queried in two ways: API and sql queries\"\n",
    "print \"First method: API\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|first_name|gender|\n",
      "+----------+------+\n",
      "|      Mark|  Male|\n",
      "|  Patricia|Female|\n",
      "|   Gregory|  Male|\n",
      "|      Mark|  Male|\n",
      "|  Jennifer|Female|\n",
      "|       Ann|Female|\n",
      "|    Howard|  Male|\n",
      "|    Samuel|  Male|\n",
      "|    Jeremy|  Male|\n",
      "|      Judy|Female|\n",
      "|    Robert|  Male|\n",
      "| Christina|Female|\n",
      "|       Joe|  Male|\n",
      "|     Jesse|  Male|\n",
      "|      Sara|Female|\n",
      "|     Terry|  Male|\n",
      "|     Kelly|Female|\n",
      "|    Nicole|Female|\n",
      "|    Daniel|  Male|\n",
      "|  Benjamin|  Male|\n",
      "+----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df['first_name'], df['gender']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|Female|  479|\n",
      "|  Male|  521|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df[\"gender\"]).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               email|count|\n",
      "+--------------------+-----+\n",
      "| jbaker4@auda.org.au|    1|\n",
      "|   jyoung6d@sohu.com|    1|\n",
      "|  ahall9c@flickr.com|    1|\n",
      "|croberts9k@webede...|    1|\n",
      "|   ckingcl@apple.com|    1|\n",
      "|fcoopernt@google.com|    1|\n",
      "|janderson5h@oaic....|    1|\n",
      "|dmatthewsb7@image...|    1|\n",
      "| jpowellbk@wiley.com|    1|\n",
      "|arogersel@rediff.com|    1|\n",
      "| ameyer16@rambler.ru|    1|\n",
      "|     bsims67@msn.com|    1|\n",
      "|mjoneslf@wootheme...|    1|\n",
      "|mfranklinn9@hao12...|    1|\n",
      "|aandrewspf@redcro...|    1|\n",
      "| sanderson4r@ask.com|    1|\n",
      "|  fcoopercq@live.com|    1|\n",
      "|ladamsgf@hubpages...|    1|\n",
      "|rbellgv@bluehost.com|    1|\n",
      "|abrown36@yellowpa...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(df[\"email\"]).agg(func.count(df[\"email\"]).alias(\"count\")).orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second method: sql\n"
     ]
    }
   ],
   "source": [
    "print \"Second method: sql\"\n",
    "df.registerTempTable(\"people\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|               email|first_name|gender| id|     ip_address|last_name|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "|pbarnes1@state.tx.us|  Patricia|Female|  2|  118.198.82.58|   Barnes|\n",
      "| jbaker4@auda.org.au|  Jennifer|Female|  5|   39.74.105.41|    Baker|\n",
      "|agilbert5@reddit.com|       Ann|Female|  6|199.219.100.148|  Gilbert|\n",
      "|     jlopez9@ask.com|      Judy|Female| 10| 148.247.157.84|    Lopez|\n",
      "|cfullerb@comcast.net| Christina|Female| 12|  64.222.238.34|   Fuller|\n",
      "|swarrene@economis...|      Sara|Female| 15|139.106.252.103|   Warren|\n",
      "|    kcarrg@prlog.org|     Kelly|Female| 17|   42.81.36.143|     Carr|\n",
      "|nbellh@soundcloud...|    Nicole|Female| 18|    219.227.1.1|     Bell|\n",
      "|tgarciak@bizjourn...|   Theresa|Female| 21|243.177.142.241|   Garcia|\n",
      "|mweaverl@china.co...|  Margaret|Female| 22|    75.44.81.77|   Weaver|\n",
      "|     wsmithm@mlb.com|     Wanda|Female| 23|   116.184.26.4|    Smith|\n",
      "|  lpierceq@wikia.com|   Lillian|Female| 27|  8.128.214.103|   Pierce|\n",
      "| hharrisont@ucsd.edu|   Heather|Female| 30|185.176.240.194| Harrison|\n",
      "|    nlynchz@lulu.com|     Nancy|Female| 36|    99.8.227.53|    Lynch|\n",
      "|aturner14@macrome...|    Ashley|Female| 41|  90.221.138.42|   Turner|\n",
      "|barmstrong15@jimd...|    Bonnie|Female| 42| 217.98.101.179|Armstrong|\n",
      "| ameyer16@rambler.ru|       Amy|Female| 43|   217.88.103.9|    Meyer|\n",
      "| dmorgan1a@diigo.com|     Debra|Female| 47|  29.194.154.19|   Morgan|\n",
      "|aalexander1d@wix.com|     Alice|Female| 50| 160.189.56.190|Alexander|\n",
      "|   sperry1f@java.com|   Shirley|Female| 52|   94.183.1.219|    Perry|\n",
      "+--------------------+----------+------+---+---------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlContext.sql(\"SELECT * FROM people WHERE gender = 'Female'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We can define our own functions as well.\n",
      "+--------------------+-----+\n",
      "|              domain|count|\n",
      "+--------------------+-----+\n",
      "|         alibaba.com|    8|\n",
      "|        examiner.com|    7|\n",
      "|             163.com|    7|\n",
      "|       woothemes.com|    6|\n",
      "|             mlb.com|    6|\n",
      "|      friendfeed.com|    6|\n",
      "|             fda.gov|    6|\n",
      "|             free.fr|    6|\n",
      "|           apple.com|    6|\n",
      "|     sourceforge.net|    6|\n",
      "|         cornell.edu|    6|\n",
      "|            lulu.com|    6|\n",
      "|             usa.gov|    5|\n",
      "|        cbslocal.com|    5|\n",
      "|pagesperso-orange.fr|    5|\n",
      "|             pbs.org|    5|\n",
      "|        gravatar.com|    5|\n",
      "|            ucla.edu|    5|\n",
      "|             nyu.edu|    5|\n",
      "|       webeden.co.uk|    5|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"We can define our own functions as well.\"\n",
    "domain = func.udf(lambda s: s.split('@')[1], StringType())\n",
    "df.select(domain(df.email).alias('domain'))\\\n",
    ".groupBy('domain').agg(func.count('domain').alias(\"count\")).orderBy(\"count\", ascending=False).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
